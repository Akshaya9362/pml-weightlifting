---
title: "Classifying Weight Lifting Techniques Using Random Forests"
author: "Akshaya Bhat"
date: "2025-10-31"
output: html_document
---

## 📘 Introduction

Using data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants, we aim to classify **how well** the exercise was performed.  
This project builds a predictive model to identify the manner in which participants performed barbell lifts, using the **“classe”** variable as the outcome.

---

## 📥 Load Required Packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(randomForest)
library(rpart)
library(ggplot2)
library(dplyr)
```

---


```{r}
# Load the data
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl))
testing  <- read.csv(url(testUrl))

dim(training)
dim(testing)

```

---

```{r}
# Data cleaning and preparation
# Remove columns with too many missing values

training <- training[, colSums(is.na(training)) == 0]
testing  <- testing[, colSums(is.na(testing)) == 0]

# Remove non-predictor identifier columns

training <- training[, -(1:7)]
testing  <- testing[, -(1:7)]

# Align predictor columns between training and testing

commonCols <- intersect(names(training), names(testing))

# Keep only common columns

training <- training[, c(commonCols, "classe")]
testing  <- testing[, commonCols]

# Ensure 'classe' is factor

training$classe <- as.factor(training$classe)

cat("✅ Data cleaned and aligned.\n")
cat("Training columns:", length(names(training)), "\n")
cat("Testing columns:", length(names(testing)), "\n")
```

---

```{r}
# Convert predictors that are factors but actually numeric

for (col in names(training)) {
if (col != "classe" && is.factor(training[[col]])) {
training[[col]] <- as.numeric(as.character(training[[col]]))
}
}
for (col in names(testing)) {
if (is.factor(testing[[col]])) {
testing[[col]] <- as.numeric(as.character(testing[[col]]))
}
}
cat("✅ Converted factor predictors to numeric where applicable.\n")

```

---

```{r}
# Split into training and validation sets
set.seed(123)
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
trainData <- training[inTrain, ]
validData <- training[-inTrain, ]

cat("Training rows:", nrow(trainData), "\n")
cat("Validation rows:", nrow(validData), "\n")

```

---

```{r}
# Train Random Forest Model with Repeated Cross-Validation
set.seed(456)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, verboseIter = TRUE)

modelRF <- train(classe ~ .,
data = trainData,
method = "rf",
trControl = ctrl,
ntree = 100,
importance = TRUE)

saveRDS(modelRF, "modelRF_repcv.rds")
modelRF

```

---

```{r}
# Model Evaluation on Validation Set
# Predict on validation data

predRF <- predict(modelRF, validData)

# Confusion Matrix

confMat <- confusionMatrix(predRF, validData$classe)

cat("✅ Model Evaluation Summary\n")
cat("--------------------------------------------------\n")
cat("📈 Accuracy:", round(confMat$overall['Accuracy'] * 100, 2), "%\n")
cat("📉 Kappa:", round(confMat$overall['Kappa'], 3), "\n")
cat("--------------------------------------------------\n")

confMat$table

```

---

```{r}
# Feature Importance
varImpRF <- varImp(modelRF)
plot(varImpRF, top = 25, main = "Top 25 Important Features in Random Forest")
```

---

```{r}
# Expected Out-of-Sample Error
accuracy_val <- as.numeric(confMat$overall["Accuracy"])
oos_error_est <- 1 - accuracy_val
cat("Estimated Out-of-Sample Error:", round(oos_error_est * 100, 3), "%\n")
```

---

```{r}
# Predict on Final Test Data
finalPred <- predict(modelRF, testing)

cat("✅ Final predictions generated. Number of test cases:", length(finalPred), "\n")

# Save predictions to individual files (as required by Coursera)

dir.create("predictions_output", showWarnings = FALSE)
for (i in seq_along(finalPred)) {
write.table(finalPred[i],
file = paste0("predictions_output/problem_id_", i, ".txt"),
quote = FALSE, row.names = FALSE, col.names = FALSE)
}
cat("💾 Individual prediction files saved in 'predictions_output/' folder.\n")

# Combined predictions

write.table(finalPred, "final_predictions.txt",
row.names = FALSE, col.names = FALSE)
cat("💾 Combined predictions saved to 'final_predictions.txt'\n")

head(finalPred)

```

---

# Discussion

The Random Forest model achieved >99% accuracy on the validation data.
Given this, the estimated out-of-sample error is <1%.

Random Forests were chosen for their robustness to noise, ability to handle nonlinear relationships, and automatic feature selection via bagging and averaging.
The most important predictors were related to belt, forearm, and dumbbell sensor statistics such as roll, pitch, and yaw.

---

# Conclusion

- High model accuracy (~99%)

- Out-of-sample error <1%

- Model generalizes well with minimal overfitting

- Predictions saved in Coursera-required format (20 text files)
